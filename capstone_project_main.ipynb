{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Clustering for  Single Family Rental in Georgia State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single-Family Rental(SFR) is a label assigned to standalone rental properties. SFRs are sometimes called single-family residential properties.\n",
    "The aim of this project is to provide insight on how we can use clustering to identify outliers in the housing market, characterize and identify different kinds of anomalies, identify oportunities for investment in each neighborhood and environment. \n",
    "It provides data backed insight for an SFR Investor who wish to invest in a place with long term growth by highlighting different areas with increased gentrification correlated with property prices.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aim: Develop a system of visualisation of the different clusters in GA SFR to guide a potential investor in making optimum investment choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/eisgandar/car-prices-predict-with-ensemble-methods\n",
    "    https://www.kaggle.com/code/eisgandar/customer-segmentation-with-k-means/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import the clean dataset for Ga_listing\n",
    "gadf = pd.read_csv(\"json_rest_sep26.csv\")\n",
    "# we subset to data that concerns single family residence\n",
    "#gadf = gadf[gadf[\"Type__Detached\"]==1]\n",
    "\n",
    "gadf = gadf.iloc[:, 1:]\n",
    "lst1 = gadf[gadf[\"price\"]> 150000]  \n",
    "\n",
    "# Filter by price to retain only like single family rentals\n",
    "gadf = lst1[lst1[\"price\"]<1000000]\n",
    "gadf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Sort values by price and drop duplicates\n",
    "gadf = gadf.sort_values(by= \"price\")\n",
    "gadf.drop_duplicates(inplace=True)\n",
    "gadf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset to where there are less than 10 beds nad 9 baths, and less than 20000 sq ft of lot size\n",
    "gadf = gadf[gadf.beds<10]\n",
    "gadf = gadf[gadf.baths_full<10]\n",
    "gadf=gadf[gadf.lot_size<20000]\n",
    "gadf.price =gadf.price.apply(lambda x: x/100000)\n",
    "gadf.square_footage =gadf.square_footage.apply(lambda x: x/1000)\n",
    "gadf.year_built =gadf.year_built.apply(lambda x: x/10)\n",
    "gadf.aland_sqmi =gadf.aland_sqmi.apply(lambda x: x/10)\n",
    "#.drop(\"4yrs_max\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gadf.aland_sqmi.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict special feautes to maximum of 3 and replace square footage 0 by the mean.\n",
    "\n",
    "gadf.special_features =np.where(gadf.special_features>3, 3, gadf.special_features)\n",
    "gadf.square_footage =np.where(gadf.square_footage==0, gadf.square_footage.mean(), gadf.square_footage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualise the scatter plots of each element against price\n",
    "for ele in gadf.iloc[:, 3:32].columns :\n",
    "    print(ele.upper())\n",
    "    plt.scatter(gadf[ele], gadf.price)\n",
    "    #plt.bar(gadf[ele], gadf.price)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is used for:\n",
    "\n",
    "(1) Dimensionality Reduction\n",
    "If we have a high-dimensional dataset (the one we're working with has 44 dimenions), it is computationally more expensive to create predictions. Moreover with a greater amount of features, there's a greater risk of overfitting.\n",
    "\n",
    "(2) Visualization (in EDA). This is what we're going to do here. As it is we cannot visualise all features, (we can only visualize 3 dimensions)\n",
    "PCA allows us to visualize our dataset along two or three most important principal components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2.1 Normalisation of the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the obtained values are very large and we need to scaled to avoid dominance by these extreme values. The principal components may vary if the columns involved are not somewhat homogenous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop unneccesary columns and create new useful features.\n",
    "\n",
    "gadf3 = gadf   #.drop([ \"city_new\",\"NationalRank\", \"county_new\"], axis =1) # drop repeated columns\n",
    "\n",
    "gadf3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale the data and prepare it for PCA\n",
    "\n",
    "gadf3[\"2020pop\"] = gadf3[\"2020pop\"]/10000\n",
    "gadf3[\"2019pop\"] = gadf3[\"2019pop\"]/10000\n",
    "gadf3[\"2018pop\"] = gadf3[\"2018pop\"]/10000\n",
    "gadf3[\"2017pop\"] = gadf3[\"2017pop\"]/10000\n",
    "\n",
    "gadf3[\"pop_growth_2020\"] = (100*(gadf3[\"2020pop\"]- gadf3[\"2019pop\"])/(gadf3[\"2019pop\"]))\n",
    "gadf3[\"pop_growth_2019\"] = (100*(gadf3[\"2019pop\"]- gadf3[\"2018pop\"])/(gadf3[\"2018pop\"]))\n",
    "gadf3[\"pop_growth_2018\"] = (100*(gadf3[\"2018pop\"]- gadf3[\"2017pop\"])/(gadf3[\"2017pop\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new feature on population growth\n",
    "\n",
    "gadf3[\"4yrs_min_pop_growth\"] = list(map(lambda w,x,y,z: min(w,x,y,z),gadf3[\"pop_growth_2022\"], gadf3[\"pop_growth_2020\"],gadf3[\"pop_growth_2019\"],gadf3[\"pop_growth_2018\"]))\n",
    "     \n",
    "     \n",
    "gadf3[\"avg_growth\"] = list(map(lambda w,x,y,z: (w+x+y+z)/4,gadf3[\"pop_growth_2022\"], gadf3[\"pop_growth_2020\"],gadf3[\"pop_growth_2019\"],gadf3[\"pop_growth_2018\"]))\n",
    "     \n",
    "gadf3[\"long\"] = gadf3[\"Long\"]\n",
    "gadf3[\"lat\"] = gadf3[\"Lat\"]\n",
    "\n",
    "gadf3 = gadf3.drop([\"Long\", \"Lat\"], axis= 1)\n",
    "\n",
    "# Scale columns with very large input values with the log function.\n",
    "new_lst = ['year_built','Percapita','Median_household_income', 'Median_family_income','Numberof_households2015',  'Avg.Income/H/hold']\n",
    "for ele in new_lst:\n",
    "    gadf3[\"{}\".format(ele)] = list(map(lambda x: 0 if x==0 else np.log(abs(float(x))+ 1.01), (gadf3[\"{}\".format(ele)])))\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new feature from the old ones and drop the old ones\n",
    "gadf3[\"county\"] =gadf3[\"County\"]\n",
    "gadf3[\"Zip_Code\"] =gadf3[\"Zip Code\"]# Some of the predictors have high corelation. To avoid this multicollinearity, we need to run PCA to retain the most relevant predictors.\n",
    "#gadf3.drop([\"County\", \"Zip Code\"],axis=1)\n",
    "gadf3 = gadf3.drop([\"unit_count\",\"County\",\"Zip Code\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the value counts of each feature\n",
    "for ele in gadf3.columns:\n",
    "    print(gadf3[ele].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualise how each predictor affects the Price based on Location. We do this using seaborn package\n",
    "\n",
    "fig = plt.figure(figsize=(12, 9))\n",
    "lstc =gadf3.iloc[:, 2:31].columns\n",
    "for ele in lstc:\n",
    "    print(\"Which area has the highest\", ele.upper())\n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    print(ele.upper())\n",
    "    sns.scatterplot(gadf3.iloc[:,1],gadf3.iloc[:,0], s= list(gadf3[\"price\"].apply(lambda x: 10*(1))), c=gadf3[ele])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing data\n",
    "gadf3.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check for possible anomaly in the 2D locations with respect to each of the predictors using pyplot in Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "for ele in gadf3.iloc[:,2:31].columns:\n",
    "    print(\"Which area has the highest\", ele.upper())\n",
    "    fig = plt.figure(figsize=(13, 10))\n",
    "    plt.scatter(gadf3.iloc[:,1], gadf3.iloc[:,0],s=6, c= gadf3[\"{}\".format(ele)])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is used for dimensionality reduction / feature selection when the feature space contains too many irrelevant or redundant features. The aim is to find the intrinsic dimensionality of the data. It allows to project the data from the original 42-dimensional space into a lower dimensional space. Subsequently, we can project into a 3-dimensional space and plot the data and the clusters in this new space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries \n",
    "\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "np. set_printoptions(suppress=True)\n",
    "import seaborn as sns \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import plotly.express as px\n",
    "from math import sin, cos, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the heatmap of the features\n",
    "gadf3.corr().style.background_gradient(cmap = \"copper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "breaks = [3,12,16,21,25,28,35,38] # Deternine the number of columns to run PCA with.\n",
    "pca=PCA(n_components=2)\n",
    "exp_ratio = []\n",
    "# We will not use the longitude, latitude and price in the PCA to make sure that our result is blind to locations and price.\n",
    "data =gadf3[[\"longitude\", \"latitude\", \"price\"]] # We assume that longitude, latiude are independent predictors.\n",
    "pca.set_params(n_components=2)\n",
    "\n",
    "for i in range(6):\n",
    "    principal_components_ = pca.fit_transform(gadf3.iloc[:,breaks[i]:breaks[i+1]])\n",
    "    \n",
    "\n",
    "    data1 =gadf3[[\"longitude\", \"latitude\"]]\n",
    "    # Visualize data across the linear components\n",
    "     # Create a new dataframe for the PCA values\n",
    " \n",
    "    total_var = sum(pca.explained_variance_ratio_)*100\n",
    "    data1[\"PCA_1\"] =   list(principal_components_[:,0]) # Add the first pricipal component to the data1\n",
    "    \n",
    "    exp_ratio.append(pca.explained_variance_ratio_[0]) \n",
    "    \n",
    "    data[\"PCA_\"+\"{}\".format(i)] = list(principal_components_[:,0])\n",
    "    if pca.explained_variance_ratio_[1]>0.25:\n",
    "        data[\"PCA_2\"+\"{}\".format(i)] = list(principal_components_[:,1])\n",
    "        exp_ratio.append(pca.explained_variance_ratio_[1]) \n",
    "    #fig = plt.figure(figsize=(15, 10))\n",
    "    fig = px.scatter_3d(\n",
    "        np.array(data1), x=0, y=1, z=2, color=gadf3['price'],\n",
    "        title=f'Total Explained Variance: {total_var:.2f}%,  PCA_1:  {100*pca.explained_variance_ratio_[0]:.2f}%, PCA_2: {100*pca.explained_variance_ratio_[1]:.2f}% ',\n",
    "        labels={\"Longitude\", \"Latitude\", \"PCA_\"}\n",
    "\n",
    "    )\n",
    "    fig.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new datframe\n",
    "pca_df = data\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise any three combinations as it relates to price\n",
    "from itertools import combinations\n",
    "  \n",
    "# Get all combinations of [1, 2, 3]\n",
    "# and length 2\n",
    "comb = combinations(list(data.iloc[:,3:].columns), 3)\n",
    "  \n",
    "# Print the obtained combinations\n",
    "for i in list(comb):\n",
    "    \n",
    "    data1 =data[[\"{}\".format(i[0]), \"{}\".format(i[1]),\"{}\".format(i[2])]]\n",
    "    #print(data1.head())\n",
    "    fig = px.scatter_3d(\n",
    "        np.array(data1), x=0, y=1, z=2, color=0,\n",
    "        title=f'Variables:     {i[0]}, : {i[1]}, {i[2]}',\n",
    "        \n",
    "    )\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visulise the heatmap of the new data\n",
    "pca_df.corr().style.background_gradient(cmap = \"copper\") #get the correlations of each features in dataset. From the corrmatrix, we can see that there is low colinearity among the features.\n",
    "# That shows that the features are linearly independent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heatmap shows that multi collinearity is low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the Data With Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# transform data\n",
    "scaled = pd.DataFrame(scaler.fit_transform(data.iloc[:,3:]))\n",
    "scaled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMeans Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas the PCA seeks to represent all ùëõ features as linear combinations of a small number of eigenvectors, and does it to minimize the mean-squared error, in contrast, the K-means seeks to represent all ùëõ data vectors via small number of cluster centroids. That is, it seeks to represent them as linear combinations of a small number of cluster centroid vectors where linear combination weights must be all zero except for the single 1.\n",
    "This is also done to minimize the mean-squared error.\n",
    "Thus both K-means and PCA minimize the same objective function. The difference is that K-means has additional \"categorical\" constraint.\n",
    "\n",
    "\n",
    "Clustering helps in understanding the natural grouping in a dataset. Their purpose is to make sense to partition the data into some group of logical groupings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot Elbow method\n",
    "# K-means starts with allocating cluster centers randomly and then looks for \"better\" solutions. \n",
    "#One thing about this algorithm is that I have to give the number of clusters beforehand, so I'll be using the WCSS (elbow method) to come up with a more accurate idea.\n",
    "#plt.plot(range(2, 15), wcss)\n",
    "\n",
    "#plt.xlabel('Number of clusters')\n",
    "#plt.ylabel('WCSS') \n",
    "#plt.show()\n",
    "# From the graph we probaly need only 7 clusters. That is, after 12, clusters, the inertia stabilizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kmeans With Random init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize the cluster\n",
    "X=scaled\n",
    "x =pca_df.longitude\n",
    "y= pca_df.latitude\n",
    "z = pca_df.price\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "\n",
    "kmeans = KMeans(n_clusters=7 ,init = \"random\",random_state=0).fit(X)\n",
    "plt.scatter(x, y, c=kmeans.labels_, s=3)\n",
    "plt.title(\"Kmeans K=7 with random init\")\n",
    "plt.show()\n",
    "df_new = gadf3\n",
    "df_new[\"Cluster\"] = kmeans.labels_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kmeans with init k-means++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a different initilization with k-means++\n",
    "x =pca_df.longitude\n",
    "y= pca_df.latitude\n",
    "z = pca_df.price\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "\n",
    "kmeans = KMeans(n_clusters=7 ,init = \"k-means++\",random_state=0).fit(X)\n",
    "plt.scatter(x, y, c=kmeans.labels_, s=3)\n",
    "plt.title(\"K-means++, K=7\")\n",
    "plt.show()\n",
    "df_new = gadf3\n",
    "df_new[\"Cluster+\"] = kmeans.labels_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MiniBatchKmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Try MiniBatch\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "kmeans2 = MiniBatchKMeans(n_clusters=7, random_state=0, batch_size=21)\n",
    "kmeans2 = kmeans2.partial_fit(X)\n",
    "#kmeans2 = kmeans2.partial_fit(X[:,21:])\n",
    "kmeans2.cluster_centers_\n",
    "mb_label = kmeans2.predict(X)\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "df_new[\"mb_Cluster\"] = mb_label\n",
    "plt.scatter(x, y, c=mb_label, s=3)\n",
    "plt.title(\"MiniBatchKmeans with K=7\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agglomerative Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try Agglomerative Hierachical Clustering\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import numpy as np\n",
    " \n",
    "X = scaled\n",
    " \n",
    "# here we need to mention the number of clusters\n",
    "# otherwise the result will be a single cluster\n",
    "# containing all the data\n",
    "for linkage in (\"ward\", \"average\", \"complete\", \"single\"):\n",
    "    clustering2 = AgglomerativeClustering(linkage=linkage,affinity = 'euclidean', n_clusters=7)\n",
    "    clustering2.fit(X)\n",
    "    df_new[linkage] = clustering2.fit_predict(X)\n",
    "    fig = plt.figure(figsize=(11, 8))\n",
    "    plt.scatter(x, y, c=clustering2.labels_, s=3)\n",
    "    plt.title(\"Agglomerative Hierarchical Clustering with linkage {} K=7\".format(linkage))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# return the hierarchical clustering using Ward Method encoded as a linkage matrix (ndarray)\n",
    "# Plot the dendogram  - a method that tries to minimize the variance within each cluster.\n",
    "dendrogram = sch.dendrogram(sch.linkage(X, method  = \"ward\"))\n",
    "plt.title('Dendrogram')\n",
    "plt.xlabel('Properties')\n",
    "plt.ylabel('Euclidean distances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try DBSCAN\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "#centers =np.array(df_new.groupby(\"Cluster\").mean())#[[\"longitude\", \"latitude\"]])\n",
    "\n",
    "X, labels_true = make_blobs(\n",
    "    n_samples=len(pca_df), centers=7, cluster_std=0.1, random_state=0\n",
    ")\n",
    "X=scaled\n",
    "\n",
    "\n",
    "\n",
    "#X = StandardScaler().fit_transform(X)\n",
    "db = DBSCAN(eps=0.1, min_samples=100).fit(X)\n",
    "\n",
    "\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    "df_new[\"db_Cluster\"] = labels\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = 7\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "print(\"Estimated number of noise points: %d\" % n_noise_)\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))\n",
    "print(\"Adjusted Rand Index: %0.3f\" % metrics.adjusted_rand_score(labels_true, labels))\n",
    "print(\n",
    "    \"Adjusted Mutual Information: %0.3f\"\n",
    "    % metrics.adjusted_mutual_info_score(labels_true, labels)\n",
    ")\n",
    "print(\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(X, labels))\n",
    "fig = plt.figure(figsize=(11, 8))\n",
    "plt.scatter(x, y, c=labels, s=3)\n",
    "plt.title(\"DBSCAN Clustering with linkage  K=7\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Black removed and is used for noise instead.\n",
    "unique_labels = set(labels)\n",
    "colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.iloc[:,25:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_new.iloc[:,:].groupby(\"Cluster\").count() # These are the clusters produced by Kmeans random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_new.iloc[:,18:].groupby(\"Cluster+\").mean() # These are the clusters produced by Kmeans k-means++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.iloc[:,:].groupby(\"mb_Cluster\").count() # These are the clusters produced by Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.iloc[:,:].groupby(\"ward\").count() # These are the clusters produced by Agglomerative Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_new.iloc[:,:].groupby(\"Cluster3\").count() # AGGlomerative CLustering complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.iloc[:,:].groupby(\"db_Cluster\").count() # These are the clusters produced by MiniBatchKmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Means++ is a smart centroid initialization technique that handles issues of convergence that exists in the traditional Kmeans and the rest of the algorithm is the same as that of K-Means.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.iloc[:,25:].groupby(\"Cluster+\").mean() # These are the clusters produced by Kmeans random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize k-means++ Clusters \n",
    "for ele  in range (7):\n",
    "    df_new1 = df_new.groupby(\"Cluster+\").get_group(ele)\n",
    "    \n",
    "    x =df_new1.longitude\n",
    "    y= df_new1.latitude\n",
    "    z = df_new1.price\n",
    "    pcd=df_new1.iloc[:,:3]\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    print(\"Cluster_\", ele)\n",
    "    \n",
    "    plt.scatter(x, y, c=df_new1.groc_6km, s=3)\n",
    "    plt.title(\"Cluster_{}\".format(ele))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.groupby(\"Cluster\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_new.iloc[:,20:].groupby(\"Cluster\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation of the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the models using different scores\n",
    "\n",
    "from time import time\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def bench_k_means(kmeans, name, data, labels):\n",
    "    \"\"\"Benchmark to evaluate the KMeans initialization methods.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    kmeans : KMeans instance\n",
    "        A :class:`~sklearn.cluster.KMeans` instance with the initialization\n",
    "        already set.\n",
    "    name : str\n",
    "        Name given to the strategy. It will be used to show the results in a\n",
    "        table.\n",
    "    data : ndarray of shape (n_samples, n_features)\n",
    "        The data to cluster.\n",
    "    labels : ndarray of shape (n_samples,)\n",
    "        The labels used to compute the clustering metrics which requires some\n",
    "        supervision.\n",
    "    \"\"\"\n",
    "    t0 = time()\n",
    "    estimator = make_pipeline(StandardScaler(), kmeans).fit(data)\n",
    "    fit_time = time() - t0\n",
    "    results = [name, fit_time, estimator[-1].inertia_]\n",
    "\n",
    "    # Define the metrics which require only the true labels and estimator\n",
    "    # labels\n",
    "    clustering_metrics = [\n",
    "        metrics.homogeneity_score,\n",
    "        metrics.completeness_score,\n",
    "        metrics.v_measure_score,\n",
    "        metrics.adjusted_rand_score,\n",
    "        metrics.adjusted_mutual_info_score,\n",
    "    ]\n",
    "    results += [m(labels, estimator[-1].labels_) for m in clustering_metrics]\n",
    "\n",
    "    # The silhouette score requires the full dataset\n",
    "    results += [\n",
    "        metrics.silhouette_score(\n",
    "            data,\n",
    "            estimator[-1].labels_,\n",
    "            metric=\"euclidean\",\n",
    "            sample_size=100,\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Show the results\n",
    "    formatter_result = (\n",
    "        \"{:9s}\\t{:.3f}s\\t{:.0f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\"\n",
    "    )\n",
    "    print(formatter_result.format(*results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.iloc[:,3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the models using different scores\n",
    "n_digits=7\n",
    "\n",
    "print(82 * \"_\")\n",
    "\n",
    "print(\"init\\t\\ttime\\tinertia\\thomo\\tcompl\\tv-meas\\tARI\\tAMI\\tsilhouette\")\n",
    "\n",
    "kmeans = KMeans(init=\"k-means++\", n_clusters=n_digits, n_init=40, random_state=0)\n",
    "\n",
    "bench_k_means(kmeans=kmeans, name=\"k-means++\", data=data, labels=labels)\n",
    "kmeans = KMeans(init=\"random\", n_clusters=n_digits, n_init=40, random_state=0)\n",
    "bench_k_means(kmeans=kmeans, name=\"random\", data=data, labels=labels)\n",
    "\n",
    "pca = PCA(n_components=7).fit(data)\n",
    "kmeans = KMeans(init=pca.components_, n_clusters=7, n_init=40)\n",
    "bench_k_means(kmeans=kmeans, name=\"PCA-based\", data=data, labels=labels)\n",
    "\n",
    "print(82 * \"_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Subclusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get the groups and check the various characteristics of the groups\n",
    "for i in range(len(df_new.groupby(\"Cluster+\")[\"Cluster+\"].unique())) :   \n",
    "    Y = df_new.groupby(\"Cluster+\").get_group(i)\n",
    "    plist = [] # The plist scores the averge price in a cluster\n",
    "    low = 0.667*gadf[\"price\"].min()+ 0.333*gadf[\"price\"].max()\n",
    "    medium = 0.33*gadf[\"price\"].min()+ 0.667*gadf[\"price\"].max()\n",
    "\n",
    "    for ele in sorted(Y[\"price\"])[::-1]:\n",
    "\n",
    "        if ele <low:\n",
    "            plist.append(1)\n",
    "        if ele >=low and ele <medium:\n",
    "            plist.append(2)\n",
    "        if ele >= medium:\n",
    "            plist.append(3)\n",
    "    Y[\"plist\"] = plist\n",
    "    #dic = []\n",
    "    #dic.append([1, low, Y[\"price\"].max()])\n",
    "    fig = plt.figure(figsize=(11, 8))\n",
    "    print(\"Cluster_\",i)\n",
    "    sns.scatterplot(Y[\"longitude\"],Y[\"latitude\"],hue=Y[\"plist\"],palette=\"pastel\")\n",
    "    plt.show()\n",
    "    print(\"Summary of Group {}\".format(i), \"\\n\", Y.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the groups and check the various characteristics of the groups\n",
    "for i in range(len(df_new.groupby(\"Cluster+\")[\"Cluster+\"].unique())) :   \n",
    "    Y = df_new.groupby(\"Cluster+\").get_group(i)\n",
    "    plist = [] # The plist scores the averge price in a cluster\n",
    "    low = 0.667*gadf[\"Crime_Rating\"].min()+ 0.333*gadf[\"Crime_Rating\"].max()\n",
    "    medium = 0.33*gadf[\"Crime_Rating\"].min()+ 0.667*gadf[\"Crime_Rating\"].max()\n",
    "\n",
    "    for ele in sorted(Y[\"Crime_Rating\"])[::-1]:\n",
    "\n",
    "        if ele <low:\n",
    "            plist.append(1)\n",
    "        if ele >=low and ele <medium:\n",
    "            plist.append(2)\n",
    "        if ele >= medium:\n",
    "            plist.append(3)\n",
    "    Y[\"plist\"] = plist\n",
    "    #dic = []\n",
    "    #dic.append([1, low, Y[\"price\"].max()])\n",
    "    fig = plt.figure(figsize=(11, 8))\n",
    "    print(\"Cluster_\",i)\n",
    "    sns.scatterplot(Y[\"longitude\"],Y[\"latitude\"],hue=Y[\"plist\"],palette=\"pastel\")\n",
    "    plt.show()\n",
    "    print(\"Summary of Group {}\".format(i), \"\\n\", Y.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the groups and check the various characteristics of the groups\n",
    "for i in range(len(df_new.groupby(\"Cluster+\")[\"Cluster+\"].unique())) :   \n",
    "    Y = df_new.groupby(\"Cluster+\").get_group(i)\n",
    "    plist = [] # The plist scores the averge price in a cluster\n",
    "    low = 0.667*gadf[\"avg_5yr_sfrgrowth\"].min()+ 0.333*gadf[\"avg_5yr_sfrgrowth\"].max()\n",
    "    medium = 0.33*gadf[\"avg_5yr_sfrgrowth\"].min()+ 0.667*gadf[\"avg_5yr_sfrgrowth\"].max()\n",
    "\n",
    "    for ele in sorted(Y[\"avg_5yr_sfrgrowth\"])[::-1]:\n",
    "\n",
    "        if ele <low:\n",
    "            plist.append(1)\n",
    "        if ele >=low and ele <medium:\n",
    "            plist.append(2)\n",
    "        if ele >= medium:\n",
    "            plist.append(3)\n",
    "    Y[\"plist\"] = plist\n",
    "    #dic = []\n",
    "    #dic.append([1, low, Y[\"price\"].max()])\n",
    "    fig = plt.figure(figsize=(11, 8))\n",
    "    print(\"Cluster_\",i)\n",
    "    sns.scatterplot(Y[\"longitude\"],Y[\"latitude\"],hue=Y[\"plist\"],palette=\"pastel\")\n",
    "    plt.show()\n",
    "    print(\"Summary of Group {}\".format(i), \"\\n\", Y.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Analsis of the SubClusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"picture.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Clusters, we could infer that Clusters 0,2,5 are relatively low priced while Clusters 1,6 are relatively high priced.\n",
    "The Crime rating of Clusters 6 and 1 are the lowest, cluster 3 is moderate while cluster 0 is very high. The growth rate of Cluster 0 and 3 are the highests.\n",
    "Also Clusters 0 and 3 have maintened high Sfr growth in the last 5 years while Cluster 1 and 6 have show some decline. \n",
    "This is unconnected to Covid-19 that might have moved some higher income earners to relocate to lower income residential areas.\n",
    "Cluster 4 seems to have more amenities while Cluster 0 has fewer amenities. Clusters 0 has maintained a high population growth in the last 4 years while Cluster 4 has not been so lucky.\n",
    "\n",
    "Overall, Cluster 3 seems to be the best investment option when cost is not considered.\n",
    "On the other hand a low budget investor may consider Cluster 1 with the hope that amenities will eventually improve and Crime_Rating reduce\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ele  in range (7):\n",
    "    df_new2 = df_new.groupby(\"Cluster+\").get_group(ele)\n",
    "    \n",
    "    x =df_new2.longitude\n",
    "    y= df_new2.latitude\n",
    "    z = df_new2.price\n",
    "    #pcd=df_new1.iloc[:,:3]\n",
    "\n",
    "    #mask=point_cloud.price>=poi.price.min()\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    print(\"Cluster_\", ele)\n",
    "    #kmeans = KMeans(n_clusters=7, random_state=0).fit(X)\n",
    "    plt.scatter(x, y, c=df_new2.groc_6km, s=3)\n",
    "    plt.title(\"Cluster{}\".format(ele))\n",
    "    plt.show()\n",
    "    #df_new = gadf3\n",
    "    #df_new[\"Cluster\"] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D Representations of the Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, AffinityPropagation, DBSCAN\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Data Sets\n",
    "#gadf_new = gadf3# pd.read_csv(\"json_rest_newest.csv\")\n",
    "# we subset to data that concerns single family residence\n",
    "point_cloud = gadf3   #[(gadf_new[\"Type__Condo\"]==1) | (gadf_new[\"Type__Townhouse\"]==1) | (gadf_new[\"Type__Detached\"]==1)]\n",
    "#point_cloud[\"County\"] = point_cloud[\"county_new\"]\n",
    "#point_cloud =point_cloud.iloc[:,1:]\n",
    "point_cloud.iloc[:,20:38]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the columns to make them homogenous\n",
    "#point_cloud[\"square_footage\"] = point_cloud[\"square_footage\"].apply(lambda x: x/1000)\n",
    "#point_cloud[\"year_built\"] = point_cloud[\"year_built\"].apply(lambda x:x/100)\n",
    "#point_cloud[\"growth\"] = point_cloud[\"growth\"].apply(lambda x: 100*x)\n",
    "#point_cloud[\"price\"] = point_cloud[\"price\"].apply(lambda x: x/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x =point_cloud.longitude\n",
    "y= point_cloud.latitude\n",
    "z = point_cloud.price\n",
    "pcd=point_cloud.iloc[:,:3]\n",
    "mask=point_cloud.price>0\n",
    "spatial_query=pcd[point_cloud.price>np.mean(point_cloud.price)]\n",
    "\n",
    "\n",
    "#This is the cluster that has only 18 features.\n",
    "X= point_cloud.iloc[:,:37] #np.column_stack((x[mask], y[mask], z[mask], illuminance[mask], nb_of_returns[mask], intensity[mask]))\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "\n",
    "kmeans = KMeans(n_clusters=7, init = 'k-means++', random_state=0).fit(X)\n",
    "#plt.scatter(x[mask], y[mask], c=kmeans.labels_, s=3)\n",
    "\n",
    "plt.show()\n",
    "df_new[\"Cluster4\"] = kmeans.labels_\n",
    "#df_new.groupby(\"Cluster4\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x =point_cloud.longitude\n",
    "y= point_cloud.latitude\n",
    "z = point_cloud.price\n",
    "pcd=point_cloud.iloc[:,:3]\n",
    "mask=point_cloud.price>0\n",
    "spatial_query=pcd[point_cloud.price>np.mean(point_cloud.price)]\n",
    "\n",
    "\n",
    "#This is the cluster that has only 18 features.\n",
    "#X= point_cloud.iloc[:,:27] #np.column_stack((x[mask], y[mask], z[mask], illuminance[mask], nb_of_returns[mask], intensity[mask]))\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "\n",
    "kmeans = KMeans(n_clusters=7, init = \"k-means++\", random_state=0).fit(X)\n",
    "plt.scatter(x[mask], y[mask], c=list(kmeans.labels_), s=3)\n",
    "\n",
    "plt.show()\n",
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the result in  2D\n",
    "cols = point_cloud.iloc[:,24:29].columns\n",
    "\n",
    "for ele in cols:\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    print(ele.upper())\n",
    "    plt.scatter(point_cloud.longitude[mask],point_cloud.latitude[mask], c=point_cloud[\"{}\".format(ele)][mask], s=10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need at least 7 cluster because of the context of this study. We run Kmean to determine the optimum number of clusters. We tabulate the inertia and select the number K that has the lowest innertia.\n",
    "X=point_cloud.iloc[:,:27]#np.column_stack((x[mask], y[mask], z[mask])) # We want to minimize the inertia.\n",
    "wcss = [] \n",
    "for i in range(3, 10):\n",
    "    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    print(i,\", \", np.min(wcss))\n",
    "    #fig = plt.figure(figsize=(15, 10))\n",
    "    #plt.scatter(x[mask], y[mask], c=kmeans.labels_, s=5)\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot Elbow method\n",
    "# K-means starts with allocating cluster centers randomly and then looks for \"better\" solutions. One thing about this algorithm is that I have to give the number of clusters beforehand, so I'll be using the WCSS (elbow method) to come up with a more accurate idea.\n",
    "plt.plot(range(3, 10), wcss)\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS') \n",
    "plt.show()\n",
    "# From the graph we probaly need only 18 clusters. That is, after 18, clusters, the inertia stabilizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#This is the cluster that has only 18 features.\n",
    "X= point_cloud.iloc[:,:27] #np.column_stack((x[mask], y[mask], z[mask], illuminance[mask], nb_of_returns[mask], intensity[mask]))\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "\n",
    "kmeans = KMeans(n_clusters=7, random_state=0).fit(X)\n",
    "plt.scatter(x[mask], y[mask], c=kmeans.labels_, s=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_cloud[\"labels\"] = list(kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#point_cloud.groupby(\"labels\").get_group(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_cloud.iloc[:,39:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data3.iloc[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the crime rate in 3D\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "colors = [\"black\",\"blue\",\"green\",\"purple\",\"red\",\"#FF0800\", \"yellow\", \"#FF00FF\", \"#008000\",\"#00FF00\", \"#808000\",\"#FFFF00\",\"#006400\", \"#000080\",\"#0000ff\",\"#6495ed\", \"#0000FF\",\"#008080\",\"#00FFFF\"]\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax=Axes3D(fig)\n",
    "labels =list(range(7))\n",
    "data3 =df_new\n",
    "ax.set_xlabel('$Longitude$', fontsize=20, rotation=150)\n",
    "ax.set_ylabel('$Latitude$', fontsize=20, rotation=150)\n",
    "ax.set_zlabel('$Crime-Rating$', fontsize=30, rotation=60)\n",
    "for ele in labels:\n",
    "    for i in range(len(data3)):\n",
    "        if data3[\"Cluster\"].iloc[i,] == ele:\n",
    "            ax.scatter(data3.iloc[i,1],data3.iloc[i,0],data3.iloc[i,27],color=colors[ele])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "#colors = [\"orange\", \"blue\",\"green\",'#5CD925','#D94325',\"purple\",\"#800000\",\"#FF0000\", \"#800080\", \"#FF00FF\", \"#008000\",\"#00FF00\", \"#808000\",\"#FFFF00\",\"#006400\", \"#000080\",\"#0000ff\",\"#6495ed\", \"#0000FF\",\"#008080\",\"#00FFFF\"]\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax=Axes3D(fig)\n",
    "labels =list(range(7))\n",
    "data3 =df_new\n",
    "ax.set_xlabel('$Longitude$', fontsize=20, rotation=150)\n",
    "ax.set_ylabel('$Latitude$', fontsize=20, rotation=150)\n",
    "ax.set_zlabel('$rest- 6km?$', fontsize=30, rotation=60)\n",
    "for ele in labels:\n",
    "    for i in range(len(data3)):\n",
    "        if data3[\"Cluster\"].iloc[i,] == ele:\n",
    "            ax.scatter(data3.iloc[i,1],data3.iloc[i,0],data3.iloc[i,26],color=colors[ele])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax=Axes3D(fig)\n",
    "labels =list(range(7))\n",
    "#data3 =point_cloud\n",
    "ax.set_xlabel('$Longitude$', fontsize=20, rotation=150)\n",
    "ax.set_ylabel('$Latitude$', fontsize=20, rotation=150)\n",
    "ax.set_zlabel('$groc - 6km?$', fontsize=30, rotation=60)\n",
    "for i in range(len(point_cloud)):\n",
    "    for ele in labels:\n",
    "        if point_cloud[\"Cluster\"].iloc[i,] == ele:\n",
    "            ax.scatter(point_cloud.iloc[i,1],point_cloud.iloc[i,0],point_cloud.iloc[i,25],color=colors[ele])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "#colors = [\"red\",\"blue\",\"black\",'#5CD925','#D94325',\"purple\",\"#800000\",\"#FF0000\", \"#800080\", \"#FF00FF\", \"#008000\",\"#00FF00\", \"#808000\",\"#FFFF00\",\"#006400\", \"#000080\",\"#0000ff\",\"#6495ed\", \"#0000FF\",\"#008080\",\"#00FFFF\"]\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax=Axes3D(fig)\n",
    "labels =list(range(7))\n",
    "data3 =df_new\n",
    "ax.set_xlabel('$Longitude$', fontsize=20, rotation=150)\n",
    "ax.set_ylabel('$Latitude$', fontsize=20, rotation=150)\n",
    "ax.set_zlabel('$Clusters$', fontsize=30, rotation=60)\n",
    "for ele in labels:\n",
    "    for i in range(len(data3)):\n",
    "        if data3[\"Cluster\"].iloc[i,] == ele:\n",
    "            ax.scatter(data3.iloc[i,1],data3.iloc[i,0],data3.iloc[i,39],color=colors[ele])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.groupby(\"Cluster+\").get_group(3).iloc[:, 33:][\"county\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualisation with Geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualise the data on geopandas.\n",
    "from shapely.geometry import Point, Polygon\n",
    "import geopandas as gpd\n",
    "from geopandas import GeoDataFrame\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "\n",
    "import folium\n",
    "df_new2 = df_new\n",
    "cmap1=colors.ListedColormap([\"orange\", \"blue\",\"black\",'#5CD925','#D94325',\"purple\",\"#800000\",\"#FF0000\", \"#800080\", \"#FF00FF\", \"#008000\",\"#00FF00\", \"#808000\",\"#FFFF00\",\"#006400\", \"#000080\",\"#0000ff\",\"#6495ed\", \"#0000FF\",\"#008080\",\"#00FFFF\"])\n",
    "df_new2[\"Cluster\"] = df_new[\"Cluster\"]\n",
    "# define the geometry\n",
    "geometry = [Point(xyz) for xyz in zip(df_new2['longitude'], df_new2['latitude'])]\n",
    "gdf = GeoDataFrame(df_new2, geometry=geometry) \n",
    "\n",
    "m = gpd.GeoDataFrame(geometry=geometry, crs=\"epsg:4326\").explore(name=\"Georgia\", height=500, width=900,tiles = 'openstreetmap', zoom_start=7,min_zoom=6,max_zoom=10, cmap=cmap1)\n",
    "# plot the points, outcomes as different colors\n",
    "#folium.TileLayer('Stamen Terrain').add_to(m2)\n",
    "#folium.TileLayer('Stamen Toner').add_to(m2)\n",
    "#folium.TileLayer('Stamen Water Color').add_to(m2)\n",
    "#folium.TileLayer('cartodbpositron').add_to(m2)\n",
    "#folium.TileLayer('cartodbdark_matter').add_to(m2)\n",
    "m = gdf.explore(m =m, cmap=colors.ListedColormap([\"orange\", \"blue\",\"black\",'#5CD925','#D94325',\"purple\",\"#800000\",\"#FF0000\", \"#800080\", \"#FF00FF\", \"#008000\",\"#00FF00\", \"#808000\",\"#FFFF00\",\"#006400\", \"#000080\",\"#0000ff\",\"#6495ed\", \"#0000FF\",\"#008080\",\"#00FFFF\"]), name=\"points\")\n",
    "# add layer control so layers can be switched on / off\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine the fastest growing area by population\n",
    "    \n",
    "#gdf = gpd.read_file(gpd.datasets.get_path('naturalearth_cities'))\n",
    "\n",
    "m = gdf.explore(m =m, cmap=colors.ListedColormap([\"orange\", \"blue\",\"black\",'#5CD925','#D94325',\"purple\",\"#800000\",\"#FF0000\", \"#800080\", \"#FF00FF\", \"#008000\",\"#00FF00\", \"#808000\",\"#FFFF00\",\"#006400\", \"#000080\",\"#0000ff\",\"#6495ed\", \"#0000FF\",\"#008080\",\"#00FFFF\"]), name=\"points\")\n",
    "# add layer control so layers can be switched on / off\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "gdf['Hemisphere'] = list(map(lambda x: str(x),gdf[\"Cluster+\"]))\n",
    "cluster = gdf.explore(column='Hemisphere',tiles = \"openstreetmap\", cmap=colors.ListedColormap([\"orange\", \"blue\",\"black\",'#5CD925','#D94325',\"purple\",\"#800000\",\"#FF0000\", \"#800080\", \"#FF00FF\", \"#008000\",\"#00FF00\", \"#808000\",\"#FFFF00\",\"#006400\", \"#000080\",\"#0000ff\",\"#6495ed\", \"#0000FF\",\"#008080\",\"#00FFFF\"]))\n",
    "#folium.TileLayer('Stamen Terrain').add_to(cluster)\n",
    "folium.LayerControl().add_to(cluster)\n",
    "cluster                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine the center of the distribution \n",
    "        \n",
    "    \n",
    "#long =-84.3880\n",
    "#lat = 33.7490\n",
    "\n",
    "#gdf['Hemisphere'] = list(map(lambda x,y: 'Norte'if (x-long)**2+(y-lat)**2 <0.02  else  \"Yellow\",gdf[\"longitude\"],gdf[\"latitude\"]))\n",
    "#gdf.explore(column='Hemisphere', cmap=colors.ListedColormap(['#5CD925','#D94325',\"yellow\"]))\n",
    "\n",
    "#gdf['Hemisphere'] = list(map(lambda x,y: 'Norte'if (x-long)**2+(y-lat)**2 <0.02  else 'Sur',gdf[\"longitude\"],gdf[\"latitude\"]))\n",
    "#gdf.explore(column='Cluster', cmap=colors.ListedColormap(['green',\"red\",\"blue\",\"black\",'#D94325','#5CD925','#D94325']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[\"Median_family_income\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Determine the fastest growing area by population\n",
    "    \n",
    "#gdf = gpd.read_file(gpd.datasets.get_path('naturalearth_cities'))\n",
    "#gdf['Hemisphere'] = list(map(lambda x: 'Norte'if x<2.4   else  \"Yellow\",gdf[\"square_footage\"]))\n",
    "#gdf.explore(column='Hemisphere', cmap=colors.ListedColormap(['#5CD925','#D94325',\"yellow\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine the fastest growing area by population\n",
    "    \n",
    "#gdf = gpd.read_file(gpd.datasets.get_path('naturalearth_cities'))\n",
    "#gdf['Hemisphere'] = list(map(lambda x: 'Norte'if x >7  else  \"Yellow\",gdf[\"Median_family_income\"]))\n",
    "#gdf.explore(column='Hemisphere', cmap=colors.ListedColormap(['#5CD925',\"yellow\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine the neighbourhoods with high prices \n",
    "    \n",
    "#gdf = gpd.read_file(gpd.datasets.get_path('naturalearth_cities'))\n",
    "gdf['Hemisphere'] = list(map(lambda x: 'Norte'if x>9   else  \"Yellow\",gdf[\"price\"]))\n",
    "gdf.explore(column='Hemisphere', cmap=colors.ListedColormap(['yellow','#D94325']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine the neighbourhoods with new buildings\n",
    "    \n",
    "#gdf = gpd.read_file(gpd.datasets.get_path('naturalearth_cities'))\n",
    "gdf['Hemisphere'] = list(map(lambda x: 'Orange'if x>0 and x<2   else  \"Yellow\",gdf[\"year_built\"]))\n",
    "#gdf['Price'] = list(map(lambda x: 'Norte'if x>1000000   else  \"Yellow\",gdf[\"price\"]))\n",
    "gdf.explore('Hemisphere', cmap=colors.ListedColormap(['#FFA500',\"green\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine areas with very low average growth\n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "#gdf = gpd.read_file(gpd.datasets.get_path('naturalearth_cities'))\n",
    "gdf['Hemisphere'] = list(map(lambda x,y: 'Norte'if y<1.1   else  \"Black\",gdf[\"4yrs_min_pop_growth\"], gdf[\"avg_growth\"]))\n",
    "gdf.explore(column='Hemisphere', cmap=colors.ListedColormap(['#5CD925','black',\"yellow\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
